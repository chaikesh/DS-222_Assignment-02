{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import h5py\n",
    "import sys \n",
    "import random\n",
    "from scipy.sparse import load_npz\n",
    "import argparse \n",
    "import time\n",
    "import os\n",
    "\n",
    "data_train=load_npz('data_train.npz');\n",
    "data_train=data_train.toarray();\n",
    "\n",
    "data_test=load_npz('data_test.npz');\n",
    "data_test=data_test.toarray();\n",
    "\n",
    "y_train=np.load('y_train.npz');\n",
    "y_test=np.load('y_test.npz');\n",
    "\n",
    "def accuracy_eval(y_test,y_pred):\n",
    "    s=0;\n",
    "    \n",
    "    y_pred = np.argmax(y_pred,axis=1);\n",
    "    \n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i,y_pred[i]]>0:\n",
    "            s+=1;\n",
    "    return s/len(y_test);\n",
    "\n",
    "\n",
    "parameter_servers=[\"10.1.1.254:2225\"];\n",
    "workers=[\"10.1.1.253:2223\",\"10.1.1.252:2224\"];\n",
    "cluster = tf.train.ClusterSpec({\"ps\":parameter_servers,\"worker\":workers});\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"job_name\",\"\",\"'ps' / 'worker'\");\n",
    "tf.app.flags.DEFINE_integer(\"task_index\",0,\"Index of task within the job\");\n",
    "FLAGS=tf.app.flags.FLAGS;\n",
    "\n",
    "config=tf.ConfigProto();\n",
    "config.gpu_options.allow_growth=True;\n",
    "config.allow_soft_placement=True;\n",
    "config.log_device_placement=True;\n",
    "\n",
    "server=tf.train.Server(cluster,job_name=FLAGS.job_name,task_index=FLAGS.task_index,config=config)\n",
    "\n",
    "if FLAGS.job_name=='ps':\n",
    "    server.join();\n",
    "elif FLAGS.job_name=='worker':\n",
    "    \n",
    "    with tf.device(tf.train.replica_device_setter(\n",
    "    worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n",
    "    cluster=cluster)):\n",
    "\n",
    "        batch_size=4000;\n",
    "\n",
    "        num_examples = len(data_train);\n",
    "        num_batches_per_epoch = int(num_examples/batch_size);\n",
    "        num_epochs=100;\n",
    "        reg_constant=.001\n",
    "\n",
    "\n",
    "\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale = reg_constant)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        global_step = tf.get_variable('global_step', [], \n",
    "                                initializer = tf.constant_initializer(0), \n",
    "                                trainable = False)\n",
    "    \n",
    "        starter_learning_rate = 0.004\n",
    "        inp =tf.placeholder(shape=(None,data_train.shape[1]),dtype=tf.float64);\n",
    "        labels=tf.placeholder(shape=(None,50),dtype=tf.float64);\n",
    "        W=tf.get_variable('W',shape=(data_train.shape[1],50),dtype=tf.float64, initializer=initializer, regularizer=regularizer);\n",
    "        b=tf.get_variable('b',shape=(50,),dtype=tf.float64,regularizer=regularizer );\n",
    "        out=tf.add(tf.matmul(inp,W),b);\n",
    "        out_logits=tf.nn.softmax(out);\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES);\n",
    "\n",
    "        loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=out));\n",
    "        loss= loss + reg_constant*sum(reg_losses);\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=starter_learning_rate);\n",
    "        replicated_opt=tf.train.SyncReplicasOptimizer(opt=optimizer,total_num_replicas=len(workers)\\\n",
    "                                                      ,replicas_to_aggregate=len(workers),use_locking=True);\n",
    "        final_optimizer = replicated_opt.minimize(loss, global_step=global_step);          \n",
    "        init_token = replicated_opt.get_init_tokens_op();\n",
    "        queue_runner = replicated_opt.get_chief_queue_runner();\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        \n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "        summary_op = tf.summary.merge_all();\n",
    "        init_token = tf.global_variables_initializer();\n",
    "        sv_obj = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\\\n",
    "                            global_step=global_step,\\\n",
    "                            init_op=init_op);\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    with sv_obj.prepare_or_wait_for_session(server.target) as session:\n",
    "\n",
    "        writer = tf.summary.FileWriter(log_dir, graph=tf.get_default_graph());\n",
    "\n",
    "        cost_plot_task0=[];\n",
    "        cost_plot_task1=[];\n",
    "\n",
    "        if FLAGS.task_index == 0:\n",
    "            sv_obj.start_queue_runners(session, [queue_runner]);\n",
    "            session.run(init_token);\n",
    "        \n",
    "        \n",
    "        for curr_epoch in range(num_epochs):\n",
    "            test_accuracy = 0\n",
    "            train_cost = 0\n",
    "            start = time.time()\n",
    "\n",
    "            for batch in range(num_batches_per_epoch):\n",
    "\n",
    "                indexes = [i % num_examples for i in range(batch * batch_size, (batch + 1) * batch_size)]\n",
    "\n",
    "                batch_train_inputs = data_train[indexes];\n",
    "                batch_train_inputs=batch_train_inputs;\n",
    "                batch_train_targets=y_train[indexes];\n",
    "                feed = {inp: batch_train_inputs,\n",
    "                        labels: batch_train_targets,}\n",
    "                batch_cost, _ = session.run([loss, final_optimizer], feed)\n",
    "                train_cost += batch_cost*batch_size\n",
    "\n",
    "\n",
    "            train_cost /= num_examples;\n",
    "\n",
    "            if FLAGS.task_index==0:\n",
    "                cost_plot_task0.append(train_cost);\n",
    "            else:\n",
    "                cost_plot_task1.append(train_cost);\n",
    "\n",
    "            if(curr_epoch%1==0):\n",
    "\n",
    "                test_start=time.time();\n",
    "\n",
    "                batch_test_inputs = data_test;\n",
    "                batch_test_inputs=batch_test_inputs;\n",
    "\n",
    "                feed_test = {\n",
    "                            inp: batch_test_inputs,\n",
    "\n",
    "                            }\n",
    "\n",
    "                y_pred = session.run(out_logits,feed_test)\n",
    "                test_accuracy=accuracy_eval(y_test,y_pred);\n",
    "                test_end=time.time();\n",
    "                print(log.format(curr_epoch+1, num_epochs, train_cost, test_accuracy, time.time() - start,test_end-test_start))\n",
    "\n",
    "\n",
    "            \n",
    "                    \n",
    "                \n",
    "    sv_obj.stop();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
